{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bc664b",
   "metadata": {},
   "source": [
    "## Agent in Team \n",
    "- A single Agent can only say create a short story for us.\n",
    "- but with a team whre many agents work together towards a common goal they can help us in writing or even helping to review, edit etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dd8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93aac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "dsa_solver = AssistantAgent(\n",
    "    name = 'Complex_DSA_Solver',\n",
    "    model_client=model_client,\n",
    "    description='A DSA solver',\n",
    "    system_message=\"You give code in python to solve complex DSA problems. Give under 100 words\"\n",
    ")\n",
    "\n",
    "code_reviewer = AssistantAgent(\n",
    "    name = 'CODE_REVEIWER',\n",
    "    model_client=model_client,\n",
    "    description='A Code Reviewer',\n",
    "    system_message=\"You review the code given by the complex_dsa_solver and make sure it is optimized.Give under 10 words\"\n",
    ")\n",
    "\n",
    "code_editor = AssistantAgent(\n",
    "    name = 'CODE_EDITOR',\n",
    "    model_client=model_client,\n",
    "    description='A Code editor',\n",
    "    system_message=\"You make the code easy to understand and add comments wherever required.Give under 10 words\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42625f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : Write a simple Hello world code ?\n",
      "\n",
      " \n",
      "\n",
      "Complex_DSA_Solver : ```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "CODE_REVEIWER : That's a perfectly good, optimized \"Hello, world!\" program.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "CODE_EDITOR : OK.  Is there anything else?\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Complex_DSA_Solver : Yes,  what else would you like to do?  Please provide a specific task or problem.\n",
      "\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=[dsa_solver,code_reviewer,code_editor],\n",
    "    max_turns=4 # -----># maximum number of Message before it stops between the agents.\n",
    ")\n",
    "\n",
    "async def test_team():\n",
    "    task = TextMessage(content='Write a simple Hello world code ?',source='User')\n",
    "    \n",
    "    result = await team.run(task=task)\n",
    "\n",
    "    for each_agent_message in result.messages:\n",
    "        print(f'{each_agent_message.source} : {each_agent_message.content}' )\n",
    "        print('\\n \\n')\n",
    "\n",
    "\n",
    "await test_team()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a546147",
   "metadata": {},
   "source": [
    "## Agent Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9572b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source='user' models_usage=None metadata={} content='Write a simple Hello world code ?' type='TextMessage'\n",
      "source='Complex_DSA_Solver' models_usage=RequestUsage(prompt_tokens=25, completion_tokens=12) metadata={} content='```python\\nprint(\"Hello, world!\")\\n```\\n' type='TextMessage'\n",
      "source='CODE_REVEIWER' models_usage=RequestUsage(prompt_tokens=44, completion_tokens=16) metadata={} content='That\\'s a perfectly good, optimized \"Hello, world!\" program.\\n' type='TextMessage'\n",
      "source='CODE_EDITOR' models_usage=RequestUsage(prompt_tokens=54, completion_tokens=9) metadata={} content='OK.  Is there anything else?\\n' type='TextMessage'\n",
      "source='Complex_DSA_Solver' models_usage=RequestUsage(prompt_tokens=62, completion_tokens=33) metadata={} content='Yes,  what kind of \"else\" are you looking for?  More code examples?  A different type of task?  Please clarify your request.\\n' type='TextMessage'\n",
      "Stop reason :  Maximum number of turns 4 reached.\n"
     ]
    }
   ],
   "source": [
    "await team.reset()\n",
    "\n",
    "from autogen_agentchat.base import TaskResult\n",
    "\n",
    "async for message in team.run_stream(task=\"Write a simple Hello world code ?\"):\n",
    "    if isinstance(message, TaskResult):\n",
    "        print(\"Stop reason : \",message.stop_reason)\n",
    "    else:\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a7afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a short poem about the fall season.\n",
      "---------- TextMessage (Complex_DSA_Solver) ----------\n",
      "The leaves, a fiery, crimson hue,\n",
      "Descend, a whispered, autumn adieu.\n",
      "Crisp air, a gentle, rustling sigh,\n",
      "As summer fades and winter nigh.\n",
      "Cool breezes dance, a golden grace,\n",
      "Nature's beauty, time and space.\n",
      "\n",
      "---------- TextMessage (CODE_REVEIWER) ----------\n",
      "A lovely autumn poem!  Well-written.\n",
      "\n",
      "---------- TextMessage (CODE_EDITOR) ----------\n",
      "Thanks!  I appreciate it.\n",
      "\n",
      "---------- TextMessage (Complex_DSA_Solver) ----------\n",
      "You're welcome!  I'm glad you enjoyed it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a short poem about the fall season.', type='TextMessage'), TextMessage(source='Complex_DSA_Solver', models_usage=RequestUsage(prompt_tokens=27, completion_tokens=58), metadata={}, content=\"The leaves, a fiery, crimson hue,\\nDescend, a whispered, autumn adieu.\\nCrisp air, a gentle, rustling sigh,\\nAs summer fades and winter nigh.\\nCool breezes dance, a golden grace,\\nNature's beauty, time and space.\\n\", type='TextMessage'), TextMessage(source='CODE_REVEIWER', models_usage=RequestUsage(prompt_tokens=92, completion_tokens=11), metadata={}, content='A lovely autumn poem!  Well-written.\\n', type='TextMessage'), TextMessage(source='CODE_EDITOR', models_usage=RequestUsage(prompt_tokens=97, completion_tokens=8), metadata={}, content='Thanks!  I appreciate it.\\n', type='TextMessage'), TextMessage(source='Complex_DSA_Solver', models_usage=RequestUsage(prompt_tokens=104, completion_tokens=15), metadata={}, content=\"You're welcome!  I'm glad you enjoyed it.\\n\", type='TextMessage')], stop_reason='Maximum number of turns 4 reached.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "await team.reset()  # Reset the team for a new task.\n",
    "await Console(team.run_stream(task=\"Write a short poem about the fall season.\"))  # Stream the messages to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47445a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9500f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_1_agent_first = AssistantAgent(\n",
    "    name = 'Agent1',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the 0 and Give result as output\"\n",
    ")\n",
    "\n",
    "add_1_agent_second = AssistantAgent(\n",
    "    name = 'Agent2',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number given by previos agent and Give result as output\"\n",
    ")\n",
    "\n",
    "add_1_agent_third = AssistantAgent(\n",
    "    name = 'Agent3',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number given by previos agent and Give result as output\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed9b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "increament_team = RoundRobinGroupChat(\n",
    "    participants=[add_1_agent_first,add_1_agent_second,add_1_agent_third],\n",
    "    max_turns= 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for Agent1_efa12a46-ac89-4f42-a681-1cbe4d9dc751/efa12a46-ac89-4f42-a681-1cbe4d9dc751\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\\n', 'status': 'INVALID_ARGUMENT'}}]\n",
      "Error processing publish message for Agent2_efa12a46-ac89-4f42-a681-1cbe4d9dc751/efa12a46-ac89-4f42-a681-1cbe4d9dc751\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for Agent3_efa12a46-ac89-4f42-a681-1cbe4d9dc751/efa12a46-ac89-4f42-a681-1cbe4d9dc751\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\\n', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\\n', 'status': 'INVALID_ARGUMENT'}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m increament_team.reset()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(increament_team.run_stream())  \u001b[38;5;66;03m# Stream the messages to the console.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:518\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    516\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    519\u001b[39m     stop_reason = message.message.content\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\\n', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"d:\\_AgenticAI\\autogen\\autogen-env\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.contents: contents is not specified\\n', 'status': 'INVALID_ARGUMENT'}}]\n"
     ]
    }
   ],
   "source": [
    "await increament_team  .reset()  # Reset the team for a new task.\n",
    "await Console(increament_team.run_stream())  # Stream the messages to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092680f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
